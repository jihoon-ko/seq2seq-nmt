{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "link = 'http://m.endic.naver.com/example.nhn?sLn=kr&exampleId=%s&webCrawl=0'\n",
    "link_n = 6540\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "for i in range(link_n):\n",
    "    url = link % (i+1)\n",
    "    response = requests.get(url).text\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    korean = str(soup.find_all(class_=\"text_wrap\")[0])[24:-7]\n",
    "    eng = str(soup.find_all(class_=\"cp_trans_area\")[0])[27:-6]\n",
    "    if i % 10 == 0:\n",
    "        print((korean, eng))\n",
    "        with open('data.txt', 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "    data.append((korean, eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open(\"data.txt\", \"r\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6540\n",
      "['how', 'are', 'you', 'doing', '?']\n"
     ]
    }
   ],
   "source": [
    "N = len(data)\n",
    "print(N)\n",
    "kor, eng = [], []\n",
    "for x in data:\n",
    "    kor.append(x[0])\n",
    "    _eng = []\n",
    "    for word in x[1].split():\n",
    "        word = word.lower()\n",
    "        punc = None\n",
    "        if len(word) > 1:\n",
    "            if ord(word[-1]) < ord('a') or ord(word[-1]) > ord('z'):\n",
    "                punc = word[-1]\n",
    "                word = word[:-1]\n",
    "        _eng.append(word)\n",
    "        if punc: _eng.append(punc)\n",
    "    eng.append(_eng)\n",
    "\n",
    "print(eng[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_enc:  75\n",
      "l_dec:  33\n",
      "m_enc:  1064\n",
      "m_dec:  3719\n"
     ]
    }
   ],
   "source": [
    "dec_wtoi = {}\n",
    "dec_itow = []\n",
    "m_dec = 0\n",
    "\n",
    "enc_wtoi = {}\n",
    "enc_itow = []\n",
    "m_enc = 0\n",
    "\n",
    "l_enc = 0\n",
    "\n",
    "\n",
    "for words in kor:\n",
    "    cnt = 0\n",
    "    l_enc = max(l_enc, len(words))\n",
    "    for char in words:\n",
    "        if char not in enc_wtoi:\n",
    "            enc_wtoi[char] = m_enc\n",
    "            enc_itow.append(char)\n",
    "            m_enc += 1\n",
    "    \n",
    "enc_wtoi[' '] = m_enc\n",
    "enc_itow.append(' ')\n",
    "\n",
    "dec_wtoi['<START>'] = m_dec\n",
    "dec_itow.append('<START>')\n",
    "m_dec += 1\n",
    "\n",
    "l_dec = 0\n",
    "for words in eng:\n",
    "    l_dec = max(l_dec, len(words))\n",
    "    for word in words:\n",
    "        if word not in dec_wtoi:\n",
    "            dec_wtoi[word] = m_dec\n",
    "            dec_itow.append(word)\n",
    "            m_dec += 1\n",
    "        \n",
    "dec_wtoi['<END>'] = m_dec\n",
    "dec_itow.append('<END>')\n",
    "m_dec += 1\n",
    "\n",
    "print (\"l_enc: \", l_enc)\n",
    "print (\"l_dec: \", l_dec)\n",
    "print (\"m_enc: \", m_enc)\n",
    "print (\"m_dec: \", m_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.064e+03 1.064e+03 1.064e+03 ... 2.000e+00 1.000e+00 0.000e+00]\n",
      " [1.064e+03 1.064e+03 1.064e+03 ... 1.000e+01 4.000e+00 9.000e+00]\n",
      " [1.064e+03 1.064e+03 1.064e+03 ... 1.900e+01 1.800e+01 1.700e+01]\n",
      " ...\n",
      " [1.064e+03 1.064e+03 1.064e+03 ... 1.600e+01 3.530e+02 0.000e+00]\n",
      " [1.064e+03 1.064e+03 1.064e+03 ... 1.600e+01 3.530e+02 0.000e+00]\n",
      " [1.064e+03 1.064e+03 1.064e+03 ... 3.000e+01 1.600e+01 4.170e+02]]\n",
      "[[0.000e+00 1.000e+00 2.000e+00 ... 3.718e+03 3.718e+03 3.718e+03]\n",
      " [0.000e+00 6.000e+00 7.000e+00 ... 3.718e+03 3.718e+03 3.718e+03]\n",
      " [0.000e+00 1.200e+01 1.300e+01 ... 3.718e+03 3.718e+03 3.718e+03]\n",
      " ...\n",
      " [0.000e+00 2.240e+02 2.110e+02 ... 3.718e+03 3.718e+03 3.718e+03]\n",
      " [0.000e+00 6.590e+02 2.110e+02 ... 3.718e+03 3.718e+03 3.718e+03]\n",
      " [0.000e+00 3.500e+01 5.210e+02 ... 3.718e+03 3.718e+03 3.718e+03]]\n",
      "[[1.000e+00 2.000e+00 3.000e+00 ... 3.718e+03 3.718e+03 3.718e+03]\n",
      " [6.000e+00 7.000e+00 8.000e+00 ... 3.718e+03 3.718e+03 3.718e+03]\n",
      " [1.200e+01 1.300e+01 1.400e+01 ... 3.718e+03 3.718e+03 3.718e+03]\n",
      " ...\n",
      " [2.240e+02 2.110e+02 3.000e+00 ... 3.718e+03 3.718e+03 3.718e+03]\n",
      " [6.590e+02 2.110e+02 3.000e+00 ... 3.718e+03 3.718e+03 3.718e+03]\n",
      " [3.500e+01 5.210e+02 1.540e+02 ... 3.718e+03 3.718e+03 3.718e+03]]\n"
     ]
    }
   ],
   "source": [
    "X_enc_train = m_enc * np.ones((N, l_enc))\n",
    "X_dec_train = (m_dec - 1) * np.ones((N, l_dec+1))\n",
    "y_train = (m_dec - 1) * np.ones((N, l_dec+1))\n",
    "for i, word in enumerate(kor):\n",
    "    cnt = 0\n",
    "    for j, char in enumerate(word):\n",
    "        if char == ' ': continue\n",
    "        cnt += 1\n",
    "        X_enc_train[i, l_enc-cnt] = enc_wtoi[char]\n",
    "\n",
    "for i, words in enumerate(eng):\n",
    "    X_dec_train[i][0] = 0\n",
    "    for j, word in enumerate(words):\n",
    "        X_dec_train[i, j+1] = y_train[i, j] = dec_wtoi[word]\n",
    "\n",
    "print(X_enc_train)\n",
    "print(X_dec_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 3719)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "n_hidden = 200\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X_enc = tf.placeholder(tf.int32, [None, l_enc])\n",
    "X_dec = tf.placeholder(tf.int32, [None, None])\n",
    "y = tf.placeholder(tf.int32, [None, None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def model(X_enc, X_dec, is_training):\n",
    "    keep_prob = tf.cond(is_training, lambda:tf.constant(0.5), lambda:tf.constant(1.0))\n",
    "    if is_training is not None:\n",
    "        trainable = True\n",
    "    else:\n",
    "        trainable = False\n",
    "    embedding_enc = tf.get_variable('embedding_enc', initializer=tf.random_uniform([m_enc, n_hidden], -1.0, 1.0), trainable=trainable)\n",
    "    mt = tf.get_variable('mt', shape=[1, n_hidden], initializer=tf.zeros_initializer(), trainable=False, dtype=tf.float32)\n",
    "    embedding_enc = tf.concat([embedding_enc, mt], 0, name='concatt')\n",
    "    embedding_dec = tf.get_variable('embedding_dec', initializer=tf.random_uniform([m_dec, n_hidden], -1.0, 1.0), trainable=trainable)\n",
    "    after_embedding_enc = tf.cast(tf.nn.embedding_lookup(embedding_enc, X_enc, validate_indices=False), tf.float32)\n",
    "    after_embedding_dec = tf.cast(tf.nn.embedding_lookup(embedding_dec, X_dec, validate_indices=False), tf.float32)\n",
    "    \n",
    "    inputs = tf.layers.conv2d(inputs=tf.reshape(after_embedding_enc, [-1, l_enc, n_hidden, 1]),\n",
    "                              filters=1,\n",
    "                              kernel_size=(1, 5),\n",
    "                              trainable=trainable)\n",
    "    \n",
    "    with tf.variable_scope('encode'):\n",
    "        enc_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "        enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=keep_prob)\n",
    "        outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, after_embedding_enc, dtype=tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('decode'):\n",
    "        dec_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "        dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "        outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, after_embedding_dec, initial_state=enc_states, dtype=tf.float32)\n",
    "    \n",
    "    out = tf.layers.dense(outputs, n_hidden, activation=None, trainable=trainable)\n",
    "    out = tf.tensordot(out, embedding_dec, axes=[[2], [1]])\n",
    "    print(out.get_shape())\n",
    "    return out\n",
    "\n",
    "y_out = model(X_enc, X_dec, is_training)\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_out, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #100\n",
      "train_loss: 0.06737235\n",
      "val_loss: 0.02902684\n",
      " input:  곧 가지 않으면 안됩니다.\n",
      "answer:  i'm in a hurry to go .\n",
      " model:  i'm in a hurry to go .\n",
      "\n",
      "val_loss: 0.03054464\n",
      " input:  그런 식으로 말하면 안돼요.\n",
      "answer:  you shouldn't say things like that .\n",
      " model:  you shouldn't say things like that .\n",
      "\n",
      " input:  당신 말도 일리는 있지만.\n",
      "answer:  you have a point , but .\n",
      " model:  you have a point , but .\n",
      "\n",
      "val_loss: 0.031544395\n",
      "val_loss: 0.031003462\n",
      " input:  아무라도 바꿔 주세요.\n",
      "answer:  i may speak to anybody .\n",
      " model:  i may speak to anybody .\n",
      "\n",
      "val_loss: 0.02584194\n",
      "correct:  844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-897a9d06d388>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m                      y: y_train[rng]}\n\u001b[0;32m     30\u001b[0m         \u001b[0mth\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mstp\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jihoon\\appdata\\local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jihoon\\appdata\\local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jihoon\\appdata\\local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jihoon\\appdata\\local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jihoon\\appdata\\local\\conda\\conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "batch_size = 128\n",
    "test_num = 1000\n",
    "N_ = N - test_num\n",
    "\n",
    "indices = np.arange(N)\n",
    "np.random.shuffle(indices)\n",
    "test_indices = indices[-test_num:]\n",
    "train_indices = indices[:-test_num]\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "stp = 0\n",
    "best_counter = 0\n",
    "for epoch in range(1000):\n",
    "    # train\n",
    "    epoch_indices = np.arange(N_)\n",
    "    np.random.shuffle(epoch_indices)\n",
    "    th = 0\n",
    "    while th < N_:\n",
    "        rng = train_indices[epoch_indices[th:min(th+batch_size, N_)]]\n",
    "        feed_dict = {X_enc: X_enc_train[rng],\n",
    "                     X_dec: X_dec_train[rng],\n",
    "                     is_training: True,\n",
    "                     y: y_train[rng]}\n",
    "        th += batch_size\n",
    "        loss, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        stp += 1\n",
    "        \n",
    "    clear_output()\n",
    "    print ('Epoch #%d' % (epoch+1))\n",
    "    print ('train_loss:', loss)\n",
    "    \n",
    "    #validation\n",
    "    correct = 0\n",
    "    for k in range(0, test_num, 200):\n",
    "        y_now = np.zeros((200, 1))\n",
    "        rng = test_indices[k:k+200]\n",
    "        for i in range(1, l_dec):\n",
    "            feed_dict = {X_enc: X_enc_train[rng],\n",
    "                         X_dec: X_dec_train[rng],\n",
    "                         is_training: False}\n",
    "            res = np.array(sess.run(y_out, feed_dict=feed_dict))\n",
    "            y_now = np.argmax(res, axis=2)\n",
    "\n",
    "        feed_dict = {X_enc: X_enc_train[rng],\n",
    "                     X_dec: X_dec_train[rng],\n",
    "                     y: y_train[rng],\n",
    "                     is_training: False}\n",
    "        loss, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        print ('val_loss:', loss)\n",
    "\n",
    "        for ii in range(200):\n",
    "            i = test_indices[k + ii]\n",
    "            import random\n",
    "            print_mode = random.random() < 0.005\n",
    "            if print_mode:\n",
    "                print (\" input: \", kor[i])\n",
    "                print (\"answer: \", \" \".join(eng[i]))\n",
    "                print (\" model: \", end=\" \")\n",
    "            model_answer = []\n",
    "            for j in range(l_dec):\n",
    "                if y_now[ii, j] == m_dec - 1:\n",
    "                    break\n",
    "                model_answer.append(dec_itow[y_now[ii, j]])\n",
    "            model_answer = \" \".join(model_answer)\n",
    "            if model_answer == \" \".join(eng[i]):\n",
    "                correct += 1\n",
    "            if print_mode:\n",
    "                print(model_answer, end='\\n\\n')\n",
    "    print(\"correct: \", correct)\n",
    "    if correct >= best_counter:\n",
    "        best_counter = correct\n",
    "        saver.save(sess, \"./model_large.ckpt\")\n",
    "        \n",
    "    \"\"\"\n",
    "    for ii in range(200):\n",
    "        i = np.random.randint(0, N)\n",
    "        print (\" input: \", kor[i])\n",
    "        print (\"answer: \", \" \".join(eng[i]))\n",
    "        print (\" model: \", end=\" \")\n",
    "        y_now = [0]\n",
    "        model_answer = []\n",
    "        for j in range(l_dec):\n",
    "            res = np.array(sess.run(y_out, feed_dict={X_enc: X_enc_train[i:i+1],\n",
    "                                                      X_dec: np.array([y_now]),\n",
    "                                                      is_training: False}))\n",
    "            best_word_idx = np.argmax(res[0][j])\n",
    "            y_now.append(best_word_idx)\n",
    "            if best_word_idx == m_dec - 1:\n",
    "                break\n",
    "            model_answer.append(dec_itow[np.argmax(res[0][j])])\n",
    "        model_answer = \" \".join(model_answer)\n",
    "        if model_answer == \" \".join(eng[i]):\n",
    "            model_answer += ' <- and the answer is correct!'\n",
    "        print(model_answer, end='\\n\\n')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.setdiff1d(indices[train_indices], indices[test_indices]).shape == train_indices.shape)\n",
    "print(np.setdiff1d(indices[test_indices], indices[train_indices]).shape == test_indices.shape)\n",
    "\n",
    "# 즉, train data와 validation data는 분리되어 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
